{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba36903-ee71-4a62-9bb8-8e0270fdae64",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244c460-fa21-482a-a6e6-4216f8f74c24",
   "metadata": {},
   "source": [
    "This notebook provides two parts:\n",
    "1. **Introducing Transformers:** this section introduces the Transformers library from HuggingFace with simple tasks to retrieve contextualised embeddings from pretrained transformer models\n",
    "2. **Classification with transformers:** it constructed a neural network classifier using Transformers to classify the emotion analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e7fafa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.8/site-packages (4.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (2022.3.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "690b61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  #Get rid of warning in code\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8a9045-9d0b-4427-b91c-a085496de2a5",
   "metadata": {},
   "source": [
    "# Part 1: Introducing Transformers\n",
    "\n",
    "Load a pretrained the BERT-tiny modelmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122d993d-e48c-4dc2-bc14-2949b3d67e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel # For BERTs\n",
    "\n",
    "model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe226ec-580f-4022-97b0-e5010ddfb55e",
   "metadata": {},
   "source": [
    "# 1.1. Tokenizers\n",
    "\n",
    "Created Tokenizer object to convert raw text to a sequence of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d24395ef-aa4a-4d62-a6ea-aa97d86f42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ed12c-cb5a-48c5-b67f-7d17b0a79514",
   "metadata": {},
   "source": [
    "Showed tokenized example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73e5852f-46b3-4e28-b952-780f472b6a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'transform', '##er', 'architecture', 'is', 'widely', 'used', 'in', 'nl', '##p', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The transformer architecture is widely used in NLP.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807c966-188f-4a0b-b435-4282d5aa0201",
   "metadata": {},
   "source": [
    "Compare with the NLTK tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee63d96e-2441-4591-bff9-3a1bf6289828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'transformer', 'architecture', 'is', 'widely', 'used', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_tokens = word_tokenize(sentence)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe1f8a9-f38b-4dbf-a0ed-1c0e7991f9c1",
   "metadata": {},
   "source": [
    "Map the tokens to their IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950440ae-d58f-4675-8fb4-6114462331fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1996, 10938, 2121, 4294, 2003, 4235, 2109, 1999, 17953, 2361, 1012]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17f77c-46f4-4ab0-9eee-858fd0b237f2",
   "metadata": {},
   "source": [
    "## 1.2. Contextualised Embeddings\n",
    "\n",
    "Convert the list of IDs to a 2-D tensor with a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07007371-2125-4fb0-8511-63ade9a2f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1996, 10938,  2121,  4294,  2003,  4235,  2109,  1999, 17953,  2361,\n",
      "          1012]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "ids_tensor = torch.tensor([ids])\n",
    "\n",
    "print(ids_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508047c1-96fa-4839-9eb3-8e64e91c5919",
   "metadata": {},
   "source": [
    "Process the sequence using our model and maps the sequence of input IDs to a sequence of output vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494c8335-7c8a-47c2-b450-a3b775f2262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.6550,  0.3572, -1.8545,  ..., -2.2321, -2.4890,  0.8569],\n",
      "         [-0.7762,  0.7065, -0.4053,  ..., -1.0436, -1.4757,  1.0586],\n",
      "         [-0.0331,  0.0583, -0.5069,  ..., -3.0095, -0.8549,  0.6007],\n",
      "         ...,\n",
      "         [-0.1059,  0.2619,  0.2993,  ..., -0.9318, -2.3270,  1.5508],\n",
      "         [ 0.2457,  0.2863,  0.6015,  ..., -2.2550, -2.1556,  0.7440],\n",
      "         [ 0.6512, -0.0050,  0.4048,  ..., -1.6719, -2.0540,  0.0476]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9995, -0.0462, -0.9893,  0.8689, -0.9952,  0.6976, -0.7959, -0.8617,\n",
      "         -0.0733,  0.0631, -0.2004, -0.0188,  0.0957,  0.9973,  0.3465, -0.5966,\n",
      "         -0.0231,  0.2026, -0.5178, -0.9471,  0.9236, -0.1754, -0.5852, -0.9344,\n",
      "         -0.9866, -0.0732, -0.9979,  0.8367,  0.6465,  0.0414,  0.0751,  0.0109,\n",
      "         -0.9963, -0.0876,  0.9558,  0.9860, -0.8553,  0.1024,  0.2728, -0.9718,\n",
      "          0.8817,  0.7812, -0.9781,  0.8534, -0.9879, -0.0432, -0.9243,  0.9781,\n",
      "          0.0589,  0.9604,  0.9951, -0.7930, -0.1010,  0.9998,  0.4897,  0.9581,\n",
      "         -0.8921, -0.3732,  0.8974, -0.8067,  0.0946,  0.6253,  0.8512,  0.7500,\n",
      "          0.9389, -0.9983,  0.0902, -0.2616,  0.7745,  0.5502,  0.9998, -0.0035,\n",
      "         -0.9685, -0.0289, -0.0050, -0.9889,  0.6671, -0.0353, -0.8759, -0.1471,\n",
      "         -0.8774,  0.1050, -0.8785, -0.9993,  0.9985, -0.3990,  0.6585, -0.9918,\n",
      "         -0.8411,  0.9567, -0.6456,  0.0018, -0.9945,  0.9735,  0.3081,  0.9290,\n",
      "         -0.8975, -0.9738, -0.9883, -0.9672, -0.8038,  0.9768, -0.9417,  0.3995,\n",
      "         -0.8751,  0.1182, -0.9928, -0.7911,  0.1377, -0.1895,  0.9922,  0.5050,\n",
      "         -0.3065,  0.9948, -0.9934,  0.1128,  0.7057,  0.9215, -0.1071, -0.9760,\n",
      "         -0.2978, -0.9856, -0.9533,  0.6290, -0.9895,  0.9924,  0.9407,  0.9272]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "tensor([[ 0.6550,  0.3572, -1.8545,  ..., -2.2321, -2.4890,  0.8569],\n",
      "        [-0.7762,  0.7065, -0.4053,  ..., -1.0436, -1.4757,  1.0586],\n",
      "        [-0.0331,  0.0583, -0.5069,  ..., -3.0095, -0.8549,  0.6007],\n",
      "        ...,\n",
      "        [-0.1059,  0.2619,  0.2993,  ..., -0.9318, -2.3270,  1.5508],\n",
      "        [ 0.2457,  0.2863,  0.6015,  ..., -2.2550, -2.1556,  0.7440],\n",
      "        [ 0.6512, -0.0050,  0.4048,  ..., -1.6719, -2.0540,  0.0476]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_outputs = model(ids_tensor)\n",
    "print(model_outputs)\n",
    "embeddings = model_outputs['last_hidden_state'][0]\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee4181-7518-484f-993d-f7abffcae010",
   "metadata": {},
   "source": [
    "Retrieve the embedding vector for \"transform\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3df76db-f81e-4916-b31d-34a8341a20db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7762405   0.7064996  -0.40526924 -1.0537269   0.59963876 -1.4787806\n",
      " -0.06114499  1.0198268  -0.2592848  -1.3763579   0.15379079  1.0128253\n",
      "  0.7475134  -0.17591822  2.0003288  -1.01974    -0.7689706   0.05306914\n",
      "  0.13064745  0.19979265 -1.0313506  -0.5410429   1.0834255   0.4924941\n",
      "  2.2506452   1.3008716  -0.16233596  0.2252483   0.7293363   0.37714234\n",
      "  0.07085949  0.39800435 -0.37489364 -0.18650484  0.5223742  -2.7473822\n",
      " -0.53682196  0.35264653 -1.8976283  -0.35527742  0.07477728 -0.39572534\n",
      " -0.55448097  0.62232053  1.0455062  -2.1943061   0.40990543 -0.62277496\n",
      "  2.2192175  -0.13648552  0.8971438   0.80766904  0.18794227 -0.01698841\n",
      "  0.5216419  -0.3289478   0.07476875 -1.1039575   1.2602055   3.4293041\n",
      " -0.91396123 -1.8800958  -0.08931123 -0.79668564  0.06266132  0.69099814\n",
      " -0.7370051  -0.23590574 -0.42857686 -0.68002856 -0.619341    0.01592773\n",
      "  1.6605315   0.6648323  -1.6665549   2.1701157   0.7972164  -0.5222843\n",
      "  0.6280751  -0.41740742 -0.11712625 -1.3964862  -0.486962   -0.00932808\n",
      " -0.06558101 -1.1050279  -0.42741525 -0.2901256   2.4332066  -0.7398764\n",
      " -0.8465004   0.2824859  -1.8344122   1.5181533  -0.88390917  1.8521158\n",
      "  0.7205519  -0.6508573  -0.87610894  1.8371847   0.63478297 -0.8092327\n",
      "  0.06354943 -1.2577463   1.4600728  -2.0344598  -1.2294674  -0.03929336\n",
      "  0.08280373 -2.0118237  -1.2031769   0.38175339  0.02157796 -0.01494008\n",
      " -0.8913153   0.16130324  0.4589485  -1.739403    0.3124293   0.59614664\n",
      "  0.8450618   1.4435822   1.0722032   0.27800053 -4.068773   -1.043593\n",
      " -1.4757131   1.0586451 ]\n",
      "The BERT-tiny embeddings have 128 dimensions.\n"
     ]
    }
   ],
   "source": [
    "emb = embeddings[1] \n",
    "\n",
    "# convert it to a numpy array so we can perform various operations on it later on\n",
    "emb = emb.detach().numpy()\n",
    "\n",
    "print(emb)\n",
    "print(f'The BERT-tiny embeddings have {emb.shape[0]} dimensions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1d779-8ec7-462a-895b-e58a88d9bb63",
   "metadata": {},
   "source": [
    "Retrieve the embedding for \"architecture\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "827ca93a-d80d-4655-a417-6c6c6d1e07f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03308459  0.05834484 -0.50693285 -1.5424432   0.4935105   0.18124032\n",
      " -0.8694352  -0.11085375 -0.32444605 -1.2748554   0.52261645  1.8194501\n",
      "  0.13458532 -0.7986219   0.7316742  -0.7354131  -1.923584   -1.2328044\n",
      "  1.1261138   1.7006457   0.2257041  -0.61980444  1.200127   -0.08013602\n",
      "  1.6121459   1.5160662  -0.37660006  0.6972985   0.5798398  -0.8779775\n",
      "  0.51046014 -1.5074801  -1.1062824  -0.49653316  0.54861885 -1.2865149\n",
      " -0.37604594  0.45097703 -1.6104168  -0.16055688  0.4270874   1.4837753\n",
      "  0.11611927  0.02534518  0.35540295 -2.0517945   0.28007472 -0.65418786\n",
      "  2.6423407   0.21391019  0.3894924   0.83619374  0.9382673  -1.1972377\n",
      " -0.16927071 -0.7794073  -0.64581025 -1.209792    0.27398497  2.989087\n",
      " -0.5239233  -1.4748845  -0.78227526 -1.2025111  -0.47365758  0.18987575\n",
      " -0.47654122 -1.0631106  -0.6145147  -1.708671   -1.2677417  -0.64487517\n",
      "  1.5999359  -0.5242034  -1.2763854   0.81190556 -0.5610492   0.3413566\n",
      "  1.9631462   0.1756343  -0.6325791  -2.0699687   0.5209454  -0.91075116\n",
      "  0.54675055  0.37865505 -0.4221778   0.43470162  0.640409   -1.4471098\n",
      "  0.8285274  -0.19859253 -1.3807534   1.9289292  -0.6880362   2.3697157\n",
      "  0.32574043  0.8792467   0.76719075  1.8805196   0.83423305 -0.30083615\n",
      " -0.20774628  0.53041434  1.5965674  -1.4629706  -0.06511879 -0.05521546\n",
      "  0.90960765 -1.1911863  -0.5864642   0.61992013 -0.5044853   0.7044363\n",
      "  0.430947    1.1584575  -0.02526092 -1.0062225  -0.15129575 -0.2915379\n",
      " -1.1487942   1.8422754   1.49917    -0.8209472  -2.8941321  -3.009529\n",
      " -0.85485375  0.6007444 ]\n",
      "The BERT-tiny embeddings have 128 dimensions.\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "#Retrieve the word \"architecture\" in the embedding vector which is index 2 in the last hidden layer of the model\n",
    "emb2 = embeddings[2] \n",
    "\n",
    "emb_architecture = emb2.detach().numpy() #convert it in to numpy array\n",
    "\n",
    "print(emb_architecture)\n",
    "print(f'The BERT-tiny embeddings have {emb_architecture.shape[0]} dimensions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c79402-f6b7-4fc3-9f47-50fcffccbe68",
   "metadata": {},
   "source": [
    "Tokenized Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c03abaed-6738-44ab-a7dd-7775743f99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2016,  2441,  1996,  2338,  2000,  3931,  4261,  1998,  2211,\n",
      "          2000,  3191, 12575,  1012,   102,     0,     0,     0],\n",
      "        [  101,  2116,  8141,  2424,  1996,  2034,  2338,  1997,  1037,  6925,\n",
      "          1997,  2048,  3655,  2000,  2022, 16801,  1012,   102],\n",
      "        [  101,  1045,  2064,  2338,  9735,  2005,  1996,  4164,  2279,  2733,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  2610,  2359,  2000,  2338,  2032,  2005,  4439,  2205,\n",
      "          3435,  1012,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  1045,  2064,  3914,  9735,  2005,  1996,  4164,  2279,  2733,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"She opened the book to page 37 and began to read aloud.\",\n",
    "    \"Many readers find the first book of A Tale of Two Cities to be confusing.\",\n",
    "    \"I can book tickets for the concert next week.\",\n",
    "    \"The police wanted to book him for driving too fast.\",\n",
    "    \"I can reserve tickets for the concert next week.\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")  \n",
    "\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b8a3bb",
   "metadata": {},
   "source": [
    "The special padding tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abd4fa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] she opened the book to page 37 and began to read aloud. [SEP] [PAD] [PAD] [PAD]\n",
      "[CLS] many readers find the first book of a tale of two cities to be confusing. [SEP]\n",
      "[CLS] i can book tickets for the concert next week. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] the police wanted to book him for driving too fast. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] i can reserve tickets for the concert next week. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "#print the special padding tokenizer\n",
    "for id in model_inputs['input_ids']: #acess the input_ids for each sentence\n",
    "    print(tokenizer.decode(id)) #print the special padding tokenizer of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b888b7cf-54d1-450b-9431-e64629177d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inputs is a dictionary, so to provide the arguments to model(), \n",
    "# we use the double star to unpack the dictionary so that each key in the dictionary is\n",
    "# an argument to model() and each value is the value of the argument. \n",
    "model_outputs = model(**model_inputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8eb74-ce63-459c-9fb3-05fd2768ddf8",
   "metadata": {},
   "source": [
    "Obtain the contextualised word embeddings for 'book' and 'reserve' in the example sentences using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9d7e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contextualised word embeddings for 'book' in sentence1 :\n",
      " [-1.2332301e+00  8.1168419e-01  3.3068866e-02 -9.6326435e-01\n",
      "  9.5280731e-01  6.0104215e-01  5.9031653e-01  4.6202838e-02\n",
      " -9.0080577e-01  1.8362245e-01 -1.0318942e+00  7.1658599e-01\n",
      "  6.8435967e-01 -1.6353027e+00  1.4162868e+00 -1.5789974e+00\n",
      " -1.0162764e+00 -8.1221992e-01 -1.1755352e+00  1.3912032e+00\n",
      " -3.2350945e-01  2.6244619e-01  8.1340665e-01  1.8822510e+00\n",
      "  4.3167609e-01 -4.5490599e-01  2.0257646e-01  1.3786050e+00\n",
      "  5.2285039e-01 -4.3345994e-01 -1.3670484e+00  6.4454842e-01\n",
      " -1.1654184e+00 -5.2278471e-01  1.8271497e+00 -1.4007516e+00\n",
      "  6.0561180e-01 -6.8775363e-02 -5.3152099e+00 -1.2173519e+00\n",
      "  3.3846161e-01  5.3339905e-01  1.9286759e-01 -1.7144172e+00\n",
      "  6.1394453e-01 -1.1065278e+00 -9.8903686e-01  5.8604980e-01\n",
      " -2.7529672e-03  5.3680557e-01  2.0090744e+00 -1.5506877e-01\n",
      "  3.2932907e-02 -6.0485923e-01 -8.7023741e-01 -6.7914152e-01\n",
      "  1.0176802e+00  1.5285075e-01 -5.8332813e-01  1.0765169e+00\n",
      " -1.1095304e+00 -1.2091572e-01 -1.3570772e+00  2.9702428e-01\n",
      " -3.5143819e-01  3.7133330e-01  3.4216538e-01 -8.4596044e-01\n",
      " -9.1926146e-01 -1.7616180e-01 -2.1052049e-01  1.9231206e-02\n",
      "  1.5059645e-01  7.1476328e-01 -2.5724189e+00 -3.9568141e-01\n",
      "  6.0307026e-01  1.6495938e+00  1.1461337e+00 -1.6460679e-03\n",
      "  3.2499954e-02 -1.4833251e+00  6.9686341e-01  4.8937634e-01\n",
      "  6.7429286e-01 -2.8220642e-01 -1.4828503e+00  7.5249034e-01\n",
      " -1.5379913e-01  4.0611914e-01  1.4207512e+00  1.1581841e+00\n",
      " -1.0939879e+00  2.3692186e+00 -3.9497375e-01  1.1288866e+00\n",
      "  1.1696559e+00 -2.8515774e-01 -4.5575091e-01  6.8006504e-01\n",
      " -5.6521796e-02  9.8260683e-01  1.2608179e+00 -1.6605514e-01\n",
      "  3.3214631e+00 -1.1686217e+00 -6.3567448e-01  1.1376828e+00\n",
      " -1.7532828e-01  4.7307062e-01 -8.7139052e-01 -5.2635962e-01\n",
      "  9.2090410e-01 -2.4721723e+00 -1.6935632e+00 -2.4656871e-01\n",
      " -3.5074577e-03 -1.8285707e-02  1.8079571e-01  1.6686776e+00\n",
      " -2.1882960e-01  2.1575862e-01  3.3332911e-01 -2.1033459e+00\n",
      " -1.6559784e+00 -1.4578856e+00 -2.0215092e+00  9.0988952e-01]\n",
      "The contextualised word embeddings for 'book' in sentence2 :\n",
      " [-1.4857628  -1.1004711   0.7267375  -0.29128742  0.16015033 -0.6811366\n",
      "  0.56675947  0.9565494  -0.64107853  1.5715171  -0.15469818  0.97349155\n",
      "  1.0809695  -1.088944    1.1665032  -1.095951   -0.81218773 -0.74117184\n",
      " -0.6255653   1.2015009  -0.4824683  -1.0243131   0.60235566  0.8790093\n",
      "  1.9516889  -1.378407    0.14377159  1.6491382   0.5284657   0.44335362\n",
      " -1.1597453  -1.3656831  -1.2217218  -0.20371759  1.5924263  -1.5134962\n",
      "  1.8737375   1.2996961  -4.282632   -0.01702073 -0.31433323  0.59660774\n",
      "  0.34750733 -0.2621985  -0.32743585 -0.5701513  -1.6936522   0.7415084\n",
      "  1.2109126  -0.43889326  2.0636327   0.37926096  0.8813404  -0.7736907\n",
      " -1.0209578  -1.5987632   0.7519342   0.1648013  -0.15313482  0.44985428\n",
      " -0.9138774   0.19939815 -0.77101904 -0.02875824 -1.5858986   0.5619777\n",
      " -0.8049998  -0.07715141 -0.6546203  -0.18804234 -1.3935429  -0.6265432\n",
      "  0.561839    0.7445551  -1.5874504   0.19051418 -0.01295233  0.6061991\n",
      "  0.80191565  0.93929744  0.74793077 -1.335568    0.36744043  1.1370733\n",
      "  0.9245046  -0.7571103  -1.8531243   0.60759413  2.0255296   0.52606297\n",
      "  1.5445364  -0.4676984  -0.9719975   1.5093771   0.01295133  0.64305204\n",
      " -0.34094632  0.11854832  0.19633806  0.79846096  1.0739053   1.152482\n",
      "  0.4830858   0.5525862   2.3211572  -0.39177236 -0.51633275 -0.00844247\n",
      "  1.2318816  -0.62449914 -1.2787238   0.33176744  0.16972035 -2.121219\n",
      " -2.2200673  -1.1727271  -0.02327784  0.08553892 -0.81189     0.8314485\n",
      " -0.31311613  1.6198019  -1.1357372  -0.43650746 -2.175287   -3.0469398\n",
      " -1.4896654   0.54714197]\n",
      "The contextualised word embeddings for 'book' in sentence3 :\n",
      " [-2.0119507   0.22263041 -0.7807435  -0.918546    1.5750216  -0.02517059\n",
      "  0.74157786  1.1578988  -0.0097232  -0.7701321   0.82169193  0.3466198\n",
      " -0.25622702 -0.9380762   1.5025954  -1.3644631  -0.92822564 -1.0403668\n",
      " -2.4207609   2.4211395   0.79094636 -0.8187147   0.5590284   0.03841289\n",
      "  0.9156225  -0.9897065   0.73877305  0.22517347  0.79239714  1.1319927\n",
      " -1.8225459  -1.1386873  -0.84529704 -0.7081246   1.6915115  -1.0518444\n",
      "  0.41554695 -0.76416737 -1.4686168  -0.714008    0.08668965  0.99039036\n",
      " -0.521429    0.85194963 -0.42419413 -1.3672361  -0.17754844  0.40014303\n",
      " -0.4663796   0.88230073  0.9427978   0.8768064   1.8455268   0.530138\n",
      " -0.7294019   0.03712973  0.53553796 -0.16488132 -0.48460546  1.2207726\n",
      "  0.9856576  -0.41092658 -2.3224883  -0.7162078  -0.77048814  0.6296714\n",
      " -0.15816993  1.1407448  -0.58843374  0.31699848 -1.685115   -0.791362\n",
      "  0.18644601 -1.1474985  -2.7472954  -0.33756438 -0.05953991  1.9982245\n",
      "  1.6841129   0.32286963 -0.62651026  0.6037007   1.0961806   0.55446994\n",
      "  0.33249906  0.44042102 -0.43867022 -0.7812953   0.304469   -0.2880557\n",
      "  1.5385581   0.93639    -1.7211413   2.012152    0.15904772  0.26220408\n",
      "  0.36789212  0.46676072 -0.32205498  0.23455606 -0.7411485   0.5770419\n",
      "  0.20313539 -1.0404075   2.1990874  -1.7109789  -0.5262561   0.08780424\n",
      "  0.11483578  0.5864107  -0.1891346  -0.6497142   3.1317778  -0.33634743\n",
      " -0.14205988  0.10143769  0.6182763  -2.8355567   0.2577935  -1.5010173\n",
      " -0.51407915  0.9278585   0.9234637  -1.7411535  -1.368566   -1.9083185\n",
      " -1.4626621   0.17982936]\n",
      "The contextualised word embeddings for 'book' in sentence4 :\n",
      " [-1.0335481   0.73779964 -0.17262156 -1.8970355   0.7327111  -0.7076199\n",
      "  0.63744664  1.4018698  -0.28935322 -0.56807864  0.81199944  0.9600729\n",
      "  0.06665397 -0.41197428  1.3523136  -0.23246336 -2.1443362   1.5767565\n",
      " -0.90051645  0.48633146 -0.8766551  -0.31798273  0.761761    2.2556446\n",
      "  0.24654353  0.14157292 -0.521308    1.2035935   0.24553181 -0.5471736\n",
      " -0.88720423 -2.4876528  -0.8190964  -0.15493894  2.5013733   0.37609178\n",
      "  0.7798959  -0.68895864 -2.4785829  -1.1348292   0.09979478 -0.4236428\n",
      " -1.1897085   0.07327908 -0.65405774 -1.7105771  -0.19160743  0.70916724\n",
      "  1.0486389  -0.40851912 -0.40388572  0.34817722 -0.12124822 -0.39483863\n",
      "  0.8311733  -0.73102653  1.2432792  -2.3659768   0.40214378  0.77015233\n",
      "  0.02152279 -1.1283648  -0.45656425 -1.6381893   0.64301026  1.173264\n",
      "  1.5693543   0.94522846 -2.170639   -0.77817035 -1.7105684  -0.86991125\n",
      "  1.1121047   0.33907047 -1.9039583   0.0845572  -0.03647717  0.8083123\n",
      "  1.6659353  -0.5433456  -1.6482583  -1.2993599   0.62919736  1.0689324\n",
      "  1.4320853  -0.16680789 -0.9523456   0.23911293  0.47472844  0.15636367\n",
      "  1.2101136   0.8697753  -0.786865    1.185008   -0.5883038   1.2093098\n",
      "  0.60414404  0.6008692  -0.11939719 -0.03209606 -0.03583024 -0.40768364\n",
      "  0.05512556 -1.4349225   1.9179844  -1.0366569  -1.7361689   2.0234382\n",
      "  1.2316924   0.9493136  -1.1730323   2.2054858   0.34901044 -1.1643517\n",
      "  0.12197627 -0.5774871   0.01202405 -0.21312398 -1.5750413   0.77327883\n",
      " -0.6600452   0.8005923   1.1551737  -1.3833494  -2.7836185  -1.7176244\n",
      " -0.34713382  0.9193617 ]\n",
      "The contextualised word embeddings for 'reserve' in sentence5 :\n",
      " [-1.81414700e+00  8.56750071e-01 -1.27504241e+00 -9.96055663e-01\n",
      "  1.60921884e+00  1.38884515e-01  1.05432987e-01  1.19740915e+00\n",
      " -1.95693195e-01 -1.25646710e+00  1.51997435e+00  2.32611746e-01\n",
      " -9.25752997e-01 -6.20025396e-01  1.05603051e+00 -9.94649053e-01\n",
      " -6.69256806e-01 -1.03171396e+00 -2.62146759e+00  2.11263704e+00\n",
      "  9.71420705e-01 -6.61480665e-01  4.27757621e-01 -1.73087984e-01\n",
      "  1.23122811e+00 -1.11965561e+00  7.52263784e-01  1.20270014e-01\n",
      "  9.56993937e-01  1.27007937e+00 -1.32039523e+00 -1.30937243e+00\n",
      " -5.77626884e-01 -3.44594628e-01  1.01496589e+00 -8.84488285e-01\n",
      "  1.95770815e-01 -1.24850607e+00 -1.52705967e+00 -9.29302394e-01\n",
      "  6.99580789e-01  7.56489158e-01 -3.76798928e-01  5.64617634e-01\n",
      " -3.74666572e-01 -1.31753004e+00 -6.62703663e-02  3.85479294e-02\n",
      " -3.40453535e-01  8.71962786e-01  6.13370478e-01  1.07391894e+00\n",
      "  2.46187544e+00  7.22274065e-01 -1.07451522e+00  1.77708089e-01\n",
      "  3.83760512e-01 -2.59848833e-02 -4.65185583e-01  1.05174994e+00\n",
      "  1.39488018e+00  1.42310560e-03 -2.56445527e+00 -9.03836191e-01\n",
      " -4.40661609e-01  5.12666464e-01  3.38228911e-01  1.33991277e+00\n",
      " -3.11194241e-01  4.20123816e-01 -1.93013668e+00 -5.33206761e-01\n",
      "  2.16571033e-01 -1.45938396e+00 -2.08594823e+00 -5.11635125e-01\n",
      "  1.01243630e-02  1.56148207e+00  1.09390271e+00  3.31930786e-01\n",
      " -1.03562152e+00  9.44309115e-01  2.16237664e-01  5.29036522e-01\n",
      "  2.77680218e-01  9.93416727e-01  1.22189179e-01 -1.00505638e+00\n",
      " -1.08352214e-01 -5.72832584e-01  1.85635304e+00  8.71305645e-01\n",
      " -1.39349616e+00  1.61591816e+00  2.74933279e-01  3.70244712e-01\n",
      "  6.16528213e-01  2.61637211e-01  2.15391785e-01  2.26240963e-01\n",
      " -5.31791985e-01  8.33984166e-02 -3.51321340e-01 -1.07850909e+00\n",
      "  1.94965935e+00 -1.78922772e+00 -5.57365477e-01  3.38285863e-01\n",
      "  9.49348062e-02  3.52484465e-01 -6.64918050e-02 -5.27703106e-01\n",
      "  3.20860004e+00 -1.82093158e-01 -5.37192762e-01  4.01895702e-01\n",
      "  7.09876359e-01 -2.36500239e+00  3.14070225e-01 -1.89736390e+00\n",
      " -6.88699007e-01  1.09708762e+00  1.32815838e+00 -1.95104623e+00\n",
      " -1.43819761e+00 -2.12938046e+00 -1.11089385e+00 -1.73114657e-01]\n"
     ]
    }
   ],
   "source": [
    "# Retrive hidden state values produced in the last hidden layer of the model \n",
    "emb3 = model_outputs['last_hidden_state'] #Retrive embedding vector in the last hidden layer from the model_output variable \n",
    "\n",
    "# Find the word \"book\" and \"reserve\" of each sentence in the embedding vector(emb3) by using the index of each word\n",
    "# in each sentence and extrct the value to store in the variables below and convert it ti numpy array\n",
    "sent_book1 = emb3[0][3].detach().numpy() #Extract the contextualised word embeddings for 'book' in sentence 1\n",
    "sent_book2 = emb3[1][5].detach().numpy() #Extract the contextualised word embeddings for 'book' in sentence 2\n",
    "sent_book3 = emb3[2][2].detach().numpy() #Extract the contextualised word embeddings for 'book' in sentence 3\n",
    "sent_book4 = emb3[3][4].detach().numpy() #Extract the contextualised word embeddings for 'book' in sentence 4\n",
    "sent_reserve = emb3[4][2].detach().numpy() #Extract the contextualised for 'reserve' in sentence 5\n",
    "\n",
    "print(\"The contextualised word embeddings for 'book' in sentence1 :\\n\", sent_book1)\n",
    "print(\"The contextualised word embeddings for 'book' in sentence2 :\\n\", sent_book2)\n",
    "print(\"The contextualised word embeddings for 'book' in sentence3 :\\n\", sent_book3)\n",
    "print(\"The contextualised word embeddings for 'book' in sentence4 :\\n\", sent_book4)\n",
    "print(\"The contextualised word embeddings for 'reserve' in sentence5 :\\n\", sent_reserve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac47b72-46a3-40ee-b081-bca17da0b49b",
   "metadata": {},
   "source": [
    "Compare these embeddings in the cell below. In a few sentences\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a9c5c1-b6b2-4df4-8729-0f0615b5e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between book in sentence 1 and 2: 0.7383215427398682\n",
      "Cosine Similarity between book in sentence 1 and 3: 0.5869025588035583\n",
      "Cosine Similarity between book in sentence 1 and 4: 0.574064314365387\n",
      "Cosine Similarity between book in sentence 2 and 3: 0.4978688061237335\n",
      "Cosine Similarity between book in sentence 2 and 4: 0.5190578103065491\n",
      "Cosine Similarity between book in sentence 3 and 4: 0.5240395069122314\n",
      "Cosine Similarity between book in sentence 4 and reserve: 0.47602763772010803\n",
      "Cosine Similarity between book in sentence 3 and reserve: 0.9565439820289612\n",
      "Cosine Similarity between book in sentence 2 and reserve: 0.3882303535938263\n",
      "Cosine Similarity between book in sentence 1 and reserve: 0.4863606095314026\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "#import cosine to compute the cosine distance between the comparing\n",
    "from scipy.spatial.distance import cosine \n",
    "\n",
    "#compute the cosine similarity between the two embeddings vectors\n",
    "#and then, useing 1- cosine to get the consine similarity \n",
    "diff_book12 = 1 - cosine(sent_book1, sent_book2) #cosine similarity the word \"book\" between sentence 1 & 2 \n",
    "diff_book13 = 1 - cosine(sent_book1, sent_book3) #cosine similarity the word \"book\" between sentence 1 & 3\n",
    "diff_book14 = 1 - cosine(sent_book1, sent_book4) #cosine similarity the word \"book\" between sentence 1 & 4 \n",
    "diff_book23 = 1 - cosine(sent_book2, sent_book3) #cosine similarity the word \"book\" between sentence 2 & 3 \n",
    "diff_book24 = 1 - cosine(sent_book2, sent_book4) #cosine similarity the word \"book\" between sentence 2 & 4 \n",
    "diff_book34 = 1 - cosine(sent_book3, sent_book4) #cosine similarity the word \"book\" between sentence 3 & 4 \n",
    "diff_bookre35 = 1 - cosine(sent_book3, sent_reserve) #cosine similarity the word \"book\" and \"reserve\" in sentence 3 & 5\n",
    "diff_bookre45 = 1 - cosine(sent_book4, sent_reserve) #cosine similarity the word \"book\" and \"reserve\" in sentence 4 & 5\n",
    "diff_bookre25 = 1 - cosine(sent_book2, sent_reserve) #cosine similarity the word \"book\" and \"reserve\" in sentence 2 & 5\n",
    "diff_bookre15 = 1 - cosine(sent_book1, sent_reserve) #cosine similarity the word \"book\" and \"reserve\" in sentence 2 & 5\n",
    "\n",
    "print(\"Cosine Similarity between book in sentence 1 and 2:\", diff_book12)\n",
    "print(\"Cosine Similarity between book in sentence 1 and 3:\", diff_book13)\n",
    "print(\"Cosine Similarity between book in sentence 1 and 4:\", diff_book14)\n",
    "print(\"Cosine Similarity between book in sentence 2 and 3:\", diff_book23)\n",
    "print(\"Cosine Similarity between book in sentence 2 and 4:\", diff_book24)\n",
    "print(\"Cosine Similarity between book in sentence 3 and 4:\", diff_book34)\n",
    "print(\"Cosine Similarity between book in sentence 4 and reserve:\", diff_bookre45)\n",
    "print(\"Cosine Similarity between book in sentence 3 and reserve:\", diff_bookre35)\n",
    "print(\"Cosine Similarity between book in sentence 2 and reserve:\", diff_bookre25)\n",
    "print(\"Cosine Similarity between book in sentence 1 and reserve:\", diff_bookre15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768029d-395e-4eb9-ac1b-7c72e81dcd01",
   "metadata": {},
   "source": [
    "# Part 2: Classification with transformers\n",
    "\n",
    "Load up the [Tweet Eval](https://huggingface.co/datasets/tweet_eval) emotion analysis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d8cd8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fcd5efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 3257 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset with 374 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset with 1421 instances loaded\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"train\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "val_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"validation\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Validation dataset with {len(val_dataset)} instances loaded\")\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"test\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")\n",
    "\n",
    "num_classes = np.unique(train_dataset['label']).size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f2f2c-8f39-49b0-8391-33773171c5a9",
   "metadata": {},
   "source": [
    "Tokenize the examples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5b94acc-77ff-434e-99d0-ec49f00fa58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ./data_cache/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-9f5fb0569fa0881f.arrow\n",
      "Loading cached processed dataset at ./data_cache/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-04132f40f00e4298.arrow\n",
      "Loading cached processed dataset at ./data_cache/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-5a9227904869e963.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(dataset):\n",
    "    model_inputs = tokenizer(dataset['text'], padding=\"max_length\", max_length=100, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ccaf0-fc74-405b-b72f-f03328d6622a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Access a tensor containing the [CLS] embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32808677-1efb-49aa-a384-cd8ca1632d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "cls_embs = model(**model_inputs)['last_hidden_state'][:, 0]\n",
    "\n",
    "print(cls_embs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff251c-ccd2-4e0c-ab2b-a1bd1cc38101",
   "metadata": {},
   "source": [
    "Create a complete model for sequence classification, based on the BERT-tiny model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99068090-6a2d-47f0-9d22-b78a6705cd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels= num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a38d4e-27a1-4c0c-b3fa-e8dc3bdf61b5",
   "metadata": {},
   "source": [
    "Train our model and freeze weights in the BERT model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8ad56b1-d6e7-4048-9cc0-71849a003363",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc27e2c-c1d4-4692-9386-f30e477293bd",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62af0a20-79a5-49ca-882a-638a68b288f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"transformer_checkpoints\",  # specify the directory where models weights will be saved a certain points during training (checkpoints)\n",
    "    num_train_epochs=3,  # change this if it is taking too long on your computer\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e5ba93c-f8c4-4542-a316-6aac4a32362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 3257\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1224\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1224' max='1224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1224/1224 00:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.248100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to transformer_checkpoints/checkpoint-500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-1000\n",
      "Configuration saved in transformer_checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1224, training_loss=1.266378190782335, metrics={'train_runtime': 33.5924, 'train_samples_per_second': 290.87, 'train_steps_per_second': 36.437, 'total_flos': 2426108032800.0, 'train_loss': 1.266378190782335, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea6f4d-23fa-4b34-9eef-35762ef0e831",
   "metadata": {},
   "source": [
    "Predictions with the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad4e689d-ebec-44c4-b81d-649079e4a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(trained_model, test_dataset):\n",
    "\n",
    "    # Pass the required items from the dataset to the model    \n",
    "    output = trained_model(attention_mask=torch.tensor(test_dataset[\"attention_mask\"]), input_ids=torch.tensor(test_dataset[\"input_ids\"]))\n",
    "        \n",
    "    # the output dictionary contains logits, which are the unnormalised scores for each class for each example:\n",
    "    pred_labs = np.argmax(output[\"logits\"].detach().numpy(), axis=1)\n",
    "    \n",
    "    gold_labs = test_dataset[\"label\"]\n",
    "    \n",
    "    return gold_labs, pred_labs\n",
    "\n",
    "# Run the prediction function to get the results:\n",
    "gold_labs, pred_labs = predict_nn(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d377fe9",
   "metadata": {},
   "source": [
    "Compute the classification metrics of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5422b320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      1.00      0.56       558\n",
      "           1       0.00      0.00      0.00       358\n",
      "           2       0.00      0.00      0.00       123\n",
      "           3       0.00      0.00      0.00       382\n",
      "\n",
      "    accuracy                           0.39      1421\n",
      "   macro avg       0.10      0.25      0.14      1421\n",
      "weighted avg       0.15      0.39      0.22      1421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report #import classification_report\n",
    "\n",
    "print(classification_report(gold_labs, pred_labs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf8564-4f98-4e3f-b1f7-b59725a0605c",
   "metadata": {},
   "source": [
    "Load the dataset to implement \"irony\" subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62aeb113-2262-4be4-b318-3791775734c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 2862 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset with 955 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset with 784 instances loaded\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"irony\", #implemet with irony\n",
    "    split=\"train\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "val_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"irony\", #implemet with irony\n",
    "    split=\"validation\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Validation dataset with {len(val_dataset)} instances loaded\")\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"irony\", #implemet with irony\n",
    "    split=\"test\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")\n",
    "\n",
    "num_classes = np.unique(train_dataset['label']).size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe0613",
   "metadata": {},
   "source": [
    "Using tokenizer to tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f6f6dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ./data_cache/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-c744650d7bba086d.arrow\n",
      "Loading cached processed dataset at ./data_cache/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-f627c69dc4e8f3f1.arrow\n",
      "Loading cached processed dataset at ./data_cache/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-c95fc07ac0a79269.arrow\n"
     ]
    }
   ],
   "source": [
    "# Using tokenizer function to tokenize the dataset  \n",
    "# use the map() method to apply the tokenizer the dataset.\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89abbb4",
   "metadata": {},
   "source": [
    "Create the model to implement in this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59f315b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /Users/gracepichar/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /Users/gracepichar/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels= num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4c255",
   "metadata": {},
   "source": [
    "Set requires_grad to be True\n",
    "\n",
    "It set unfrozen BERT layers. It means the part of the model will change during the training process.  Each of the parameters allow to learn and update in the training model. In the backpropagation, it propagates back to the embedding layer and updates the embedding layer so, it can propagate back through the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d98afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = True #unfrozen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7bebde",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdddf688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"transformer_checkpoints\",  \n",
    "    num_train_epochs=3,  \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f05d52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2862\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1074\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1074' max='1074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1074/1074 02:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.637900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to transformer_checkpoints/checkpoint-500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-1000\n",
      "Configuration saved in transformer_checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1074, training_loss=0.6502215458250135, metrics={'train_runtime': 122.9412, 'train_samples_per_second': 69.838, 'train_steps_per_second': 8.736, 'total_flos': 2130547212000.0, 'train_loss': 0.6502215458250135, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c3a467",
   "metadata": {},
   "source": [
    "Make prediction and compute the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e76de5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy_score when unfrozen: 0.6020408163265306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "#Make prediction\n",
    "gold_labs2, pred_labs2 = predict_nn(model2, test_dataset)\n",
    "\n",
    "#compute accuracy score\n",
    "print(\"The accuracy_score when unfrozen:\", accuracy_score(gold_labs2, pred_labs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d31c0c",
   "metadata": {},
   "source": [
    "## **When frozen BERT layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce10d9",
   "metadata": {},
   "source": [
    "**Using tokenizer to tokenize the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09df2b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ./data_cache/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-40e91856f3f4ab76.arrow\n",
      "Loading cached processed dataset at ./data_cache/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-75525174e5e40e62.arrow\n",
      "Loading cached processed dataset at ./data_cache/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-daaa7f0121f93492.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa10fbb",
   "metadata": {},
   "source": [
    "**Create the model to implement in this task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b725435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /Users/gracepichar/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /Users/gracepichar/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#create model for this task\n",
    "model3 = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels= num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44a39b",
   "metadata": {},
   "source": [
    "**When frozen BERT layers**\n",
    "\n",
    "Set param.requires_grad = False to fix parameter's weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a99068d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False #frozen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3e75b",
   "metadata": {},
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36608ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"transformer_checkpoints\",  \n",
    "    num_train_epochs=3,  \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad9a0def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2862\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1074\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1074' max='1074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1074/1074 01:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.639500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to transformer_checkpoints/checkpoint-500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-1000\n",
      "Configuration saved in transformer_checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1074, training_loss=0.6532815652630627, metrics={'train_runtime': 101.2777, 'train_samples_per_second': 84.777, 'train_steps_per_second': 10.605, 'total_flos': 2130547212000.0, 'train_loss': 0.6532815652630627, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model3,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ae2a5",
   "metadata": {},
   "source": [
    "**Make prediction and compute the accuracy score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5f39126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy_score when frozen: 0.5778061224489796\n"
     ]
    }
   ],
   "source": [
    "#Make prediction \n",
    "gold_labs3, pred_labs3 = predict_nn(model3, test_dataset)\n",
    "\n",
    "#compute accuracy score\n",
    "print(\"The accuracy_score when frozen:\", accuracy_score(gold_labs3, pred_labs3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db36ca",
   "metadata": {},
   "source": [
    "**Choose the sentence from the website [this page on verbal irony](https://examples.yourdictionary.com/examples-of-verbal-irony.html):**\n",
    "\n",
    "**In J. K. Rowling’s Harry Potter and the Order of the Phoenix, Harry says, \"Yeah, Quirrell was a great teacher. There was just that minor drawback of him having Lord Voldemort sticking out of the back of his head!”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11fcdc69-ce30-4eb3-afc5-4eeba41b40db",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Store the sentence in the sentence list\n",
    "sentence = [\"Yeah, Quirrell was a great teacher. There was just that minor drawback of him having Lord Voldemort sticking out of the back of his head!\"]\n",
    "\n",
    "#Use the tokenizer class to pad the sequences up to a maximum length\n",
    "model_inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6403fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the output model from hidden state\n",
    "#use the model2 from the previous task of irony when param.requires_grad = True (unfrozen)\n",
    "model_outputs = model2(**model_inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca9bed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2801,  0.2067]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23ab2d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Yeah, Quirrell was a great teacher. There was just that minor drawback of him having Lord Voldemort sticking out of the back of his head!\n",
      "tensor([[0.3807, 0.6193]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sen = ''.join(str(i) for i in sentence) #convert list to string type to get rid of \"\"\n",
    "print(\"Sentence:\", sen) #print out the sentence\n",
    "\n",
    "#using softmax function to compute the probability of irony for a sentence\n",
    "print(torch.nn.functional.softmax((model_outputs['logits']))) #probablity by using softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd9a3a",
   "metadata": {},
   "source": [
    "**The result shows the probability of irony belonging to two classes [0.3807, 0.6193].  It means that irony has a higher probability of class-label 1.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics.",
   "language": "python",
   "name": "data_analytics."
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
